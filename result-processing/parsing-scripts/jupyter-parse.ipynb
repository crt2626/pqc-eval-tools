{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to Parse Results:\n",
      "\n",
      "['BIKE-L1', 'BIKE-L3', 'BIKE-L5', 'Classic-McEliece-348864', 'Classic-McEliece-348864f', 'Classic-McEliece-460896', 'Classic-McEliece-460896f', 'Classic-McEliece-6688128', 'Classic-McEliece-6688128f', 'Classic-McEliece-6960119', 'Classic-McEliece-6960119f', 'Classic-McEliece-8192128', 'Classic-McEliece-8192128f', 'HQC-128', 'HQC-192', 'HQC-256', 'Kyber512', 'Kyber768', 'Kyber1024', 'Kyber512-90s', 'Kyber768-90s', 'Kyber1024-90s', 'sntrup761', 'FrodoKEM-640-AES', 'FrodoKEM-640-SHAKE', 'FrodoKEM-976-AES', 'FrodoKEM-976-SHAKE', 'FrodoKEM-1344-AES', 'FrodoKEM-1344-SHAKE']\n",
      "Parsing results... \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../up-results/liboqs/mem-results/machine-1/sig-mem-metrics/sig-mem-metrics-SPHINCS+-SHA256-256f-simple-0-1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 326\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m\"\"\"Main boiler plate\"\"\"\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 326\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[110], line 319\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39m# Processing the results\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mParsing results... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 319\u001b[0m process_tests(machine_num)\n\u001b[0;32m    320\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mResults have been processed - CSV files can be found in the Results Directory at the repo root\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[110], line 294\u001b[0m, in \u001b[0;36mprocess_tests\u001b[1;34m(num_machines)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m# Parsing results\u001b[39;00m\n\u001b[0;32m    293\u001b[0m speed_processing(type_speed_dir, up_speed_dir)\n\u001b[1;32m--> 294\u001b[0m memory_processing(type_mem_dir, up_mem_dir)\n\u001b[0;32m    295\u001b[0m gen_averages(type_speed_dir, type_mem_dir)\n",
      "Cell \u001b[1;32mIn[110], line 178\u001b[0m, in \u001b[0;36mmemory_processing\u001b[1;34m(type_mem_dir, up_mem_dir)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mfor\u001b[39;00m operation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m):\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m     \u001b[39m# Parsing metrics and adding results to dataframe row\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     sig_up_filename \u001b[39m=\u001b[39m sig_up_filename_pre \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m sig_alg \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(operation) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(run_count) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 178\u001b[0m     peak_metrics \u001b[39m=\u001b[39m get_peak(sig_up_filename, peak_metrics)   \n\u001b[0;32m    179\u001b[0m     new_row\u001b[39m.\u001b[39mextend((sig_alg, sig_operations[operation]))\n\u001b[0;32m    180\u001b[0m     new_row\u001b[39m.\u001b[39mextend(peak_metrics)\n",
      "Cell \u001b[1;32mIn[110], line 107\u001b[0m, in \u001b[0;36mget_peak\u001b[1;34m(mem_file, peak_metrics)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# Parsing function from OQS Profile Project\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m# Gets max memory metric from algorithm operation\u001b[39;00m\n\u001b[0;32m    106\u001b[0m peak \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m--> 107\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(mem_file, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m lines:\n\u001b[0;32m    108\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines: \n\u001b[0;32m    109\u001b[0m         \u001b[39mif\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m Detailed snapshots: [\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../up-results/liboqs/mem-results/machine-1/sig-mem-metrics/sig-mem-metrics-SPHINCS+-SHA256-256f-simple-0-1.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\"This python script will parse the results files outputed by the bash scripts so that be viewed in a easier format or further pasresed \n",
    "by python\"\"\"\n",
    "\n",
    "\"\"\"Importing modules and declaring Global Variables\"\"\"\n",
    "# Importing\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Declaring gloabl\n",
    "kem_algs = []\n",
    "sig_algs = []\n",
    "kem_operations = [\"keygen\", \"encaps\", \"decaps\"]\n",
    "sig_operations = [\"keypair\", \"sign\", \"verify\"]\n",
    "#root_dir = \"/pqc/pqc-eval-tools/\"\n",
    "root_dir = \"../../\"\n",
    "\n",
    "#***********************************************************************\n",
    "def get_algs():\n",
    "    \"\"\"Function for creating list of algorithms\"\"\"\n",
    "\n",
    "    # Setting alg text file directories\n",
    "    kem_algs_file = root_dir + \"result-processing/algs/kem-algs-list.txt\"\n",
    "    sig_algs_file = root_dir + \"result-processing/algs/sig-algs-list.txt\"\n",
    "\n",
    "    # # Getting the kem algs\n",
    "    with open(kem_algs_file, \"r\") as kem_file:\n",
    "\n",
    "        for line in kem_file:\n",
    "            line = line.strip()\n",
    "            kem_algs.append(line)\n",
    "    \n",
    "    # Getting the digital siganture algorithms\n",
    "    with open(sig_algs_file, \"r\") as alg_file:\n",
    "\n",
    "        for line in alg_file:\n",
    "            line = line.strip()\n",
    "            sig_algs.append(line)\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "def speed_processing(type_speed_dir, up_speed_dir):\n",
    "    \"\"\"Importing and processing result files\"\"\"\n",
    "\n",
    "    # Declaring initial variables\n",
    "    kem_prefix = \"test-kem-speed-\"\n",
    "    sig_prefix = \"test-sig-speed-\"\n",
    "\n",
    "    # Creating algorithm list to insert into new column\n",
    "    new_col_kem = [alg for alg in kem_algs for i in range(3)]\n",
    "    new_col_sig = [alg for alg in sig_algs for i in range(3)]\n",
    "    \n",
    "    # Reading the original csv files and formating\n",
    "    for file_count in range(1,16,1):\n",
    "\n",
    "        \"\"\"Formating Kem Files\"\"\"\n",
    "        # Loading kem file into dataframe and stiping trailing space in columns headers\n",
    "        filename_kem_pre = up_speed_dir + kem_prefix + str(file_count) + \".csv\"\n",
    "        temp_df = pd.read_csv(filename_kem_pre, delimiter=\"|\", index_col=False)\n",
    "        temp_df.columns = [col.strip() for col in temp_df.columns]\n",
    "\n",
    "        temp_df = temp_df.loc[~temp_df['Operation'].str.strip().isin(kem_algs)]\n",
    "        temp_df.to_csv(\"../test.csv\")\n",
    "        # # Removing any now that is not a kem operation\n",
    "        # for index, row in temp_df.iterrows():\n",
    "        #     print(f\"row - {index}, row contents = {row['Operation']}\")\n",
    "        #     if row['Operation'].strip() not in kem_operations:\n",
    "                \n",
    "        #         temp_df.drop(index, inplace=True)\n",
    "        # temp_df.to_csv(\"../test3.csv\", index=False)\n",
    "        \n",
    "        # Inserting new column and outputing formated csv\n",
    "        #temp_df.insert(0, \"Algorithm\", new_col_kem)\n",
    "        filename_kem = type_speed_dir + kem_prefix + str(file_count) + \".csv\"\n",
    "        temp_df.to_csv(filename_kem, index=False)\n",
    "\n",
    "        \"\"\"Formating Digital Signature Files\"\"\"\n",
    "        # Loading kem file into dataframe and stiping trailing space in columns headers\n",
    "        filename_sig_pre = up_speed_dir + sig_prefix + str(file_count) + \".csv\"\n",
    "        temp_df = pd.read_csv(filename_sig_pre, delimiter=\"|\", index_col=False)\n",
    "        temp_df.columns = [col.strip() for col in temp_df.columns]\n",
    "        temp_df = temp_df.loc[~temp_df['Operation'].str.strip().isin(sig_algs)]\n",
    "        temp_df.to_csv(\"../test.csv\")\n",
    "\n",
    "        temp_df.to_csv('../test4.csv', index=False)        \n",
    "\n",
    "        # for index, row in temp_df.iterrows():\n",
    "        #     if row['Operation'].strip() not in sig_operations:\n",
    "        #         temp_df.drop(index, inplace=True)\n",
    "\n",
    "        # Inserting new column and outputting formated csv\n",
    "        #temp_df.insert(0, 'Algorithm', new_col_sig)\n",
    "        filename_sig = type_speed_dir + sig_prefix + str(file_count) + \".csv\"\n",
    "        temp_df.to_csv(filename_sig, index=False)\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "def get_peak(mem_file, peak_metrics):\n",
    "    \"\"\"Takes the current massif.out file and gets the peak memory metrics. \n",
    "        It comes from the run_mem.py script found in OQS Profiling Project\n",
    "        https://github.com/open-quantum-safe/profiling\"\"\"\n",
    "\n",
    "    # Parsing function from OQS Profile Project\n",
    "    # Gets max memory metric from algorithm operation\n",
    "    peak = -1\n",
    "    with open(mem_file, \"r\") as lines:\n",
    "        for line in lines: \n",
    "            if line.startswith(\" Detailed snapshots: [\"):\n",
    "                match = re.search(r\"(\\d+) \\(peak\\).*\", line)\n",
    "                if match:\n",
    "                    peak = int(match.group(1))\n",
    "            if peak > 0:\n",
    "                if line.startswith('{: >3d}'.format(peak)): # remove \",\" and print all numbers except first:\n",
    "                    nl = line.replace(\",\", \"\")\n",
    "                    peak_metrics.extend(nl.split()[1:])\n",
    "                    # print(\" \".join(res))\n",
    "\n",
    "    return peak_metrics\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "def memory_processing(type_mem_dir, up_mem_dir):\n",
    "    \"\"\"Looping through all memory files and creating csv files\"\"\"\n",
    "\n",
    "    # Assigning directory varibales\n",
    "    kem_dir = up_mem_dir + \"kem-mem-metrics/\"\n",
    "    sig_dir = up_mem_dir + \"sig-mem-metrics/\"\n",
    "    kem_file_prefix = \"kem-mem-metrics\"\n",
    "    sig_file_prefix = \"sig-mem-metrics\"\n",
    "    new_row = []\n",
    "    peak_metrics = []\n",
    "\n",
    "    # file placeholders\n",
    "    filednames = [\"Algorithm\", \"Operation\", \"intits\", \"maxBytes\", \"maxHeap\", \"extHeap\", \"maxStack\"]\n",
    "\n",
    "\n",
    "    # Looping through each run count\n",
    "    for run_count in range(1,16,1):\n",
    "\n",
    "        # Creating temp dataframe\n",
    "        temp_df = pd.DataFrame(columns=filednames)\n",
    "        #Looping throuhg kem algorithms\n",
    "        for kem_alg in kem_algs:\n",
    "\n",
    "            kem_up_filename_pre = kem_dir + kem_file_prefix\n",
    "\n",
    "            #Looping the operations and adding to temp dataframe \n",
    "            for operation in range(0,3,1):\n",
    "\n",
    "                # Parsing metrics and adding results to dataframe row\n",
    "                kem_up_filename = kem_up_filename_pre + \"-\" + kem_alg + \"-\" + str(operation) + \"-\" + str(run_count) + \".txt\"\n",
    "                peak_metrics = get_peak(kem_up_filename, peak_metrics)\n",
    "                new_row.extend([kem_alg, kem_operations[operation]])\n",
    "                new_row.extend(peak_metrics)\n",
    "                \n",
    "                temp_df.loc[len(temp_df)] = new_row\n",
    "\n",
    "                # Clearing lists\n",
    "                peak_metrics.clear()\n",
    "                new_row.clear()\n",
    "\n",
    "        # Outputing kem csv file for this run\n",
    "        kem_filename = type_mem_dir + \"kem-mem-metrics-\" + str(run_count) + \".csv\"\n",
    "        temp_df.to_csv(kem_filename, index=False)\n",
    "\n",
    "\n",
    "        #Looping throuhg kem algorithms\n",
    "        for sig_alg in sig_algs:\n",
    "\n",
    "            sig_up_filename_pre = sig_dir + sig_file_prefix\n",
    "\n",
    "            #Looping the operations and adding to temp dataframe \n",
    "            for operation in range(0,3,1):\n",
    "\n",
    "                # Parsing metrics and adding results to dataframe row\n",
    "                sig_up_filename = sig_up_filename_pre + \"-\" + sig_alg + \"-\" + str(operation) + \"-\" + str(run_count) + \".txt\"\n",
    "                peak_metrics = get_peak(sig_up_filename, peak_metrics)   \n",
    "                new_row.extend((sig_alg, sig_operations[operation]))\n",
    "                new_row.extend(peak_metrics)\n",
    "                temp_df.loc[len(temp_df)] = new_row\n",
    "\n",
    "                # Clearing lists\n",
    "                peak_metrics.clear()\n",
    "                new_row.clear()\n",
    "\n",
    "        # Outputing digital signature csv file for this run\n",
    "        sig_filename = type_mem_dir + \"sig-mem-metrics-\" + str(run_count) + \".csv\"\n",
    "        temp_df.to_csv(sig_filename, index=False)\n",
    "\n",
    "\n",
    "#***********************************************************************  \n",
    "def gen_averages(type_speed_dir, type_mem_dir):\n",
    "    \"\"\"Function for generating averages csv files for all tests\"\"\"\n",
    "\n",
    "    # Declaring directories variables\n",
    "    kem_speed_file_prefix = type_speed_dir + \"test-kem-speed-\"\n",
    "    sig_speed_file_prefix = type_speed_dir + \"test-sig-speed-\"\n",
    "    kem_mem_file_prefix = type_mem_dir + \"kem-mem-metrics-\"\n",
    "    sig_mem_file_prefix = type_mem_dir + \"sig-mem-metrics-\"\n",
    "\n",
    "    # Declaring dataframes and fieldnames\n",
    "    #mem_fieldnames = [\"Algorithm\", \"Operation\", \"intits\", \"maxBytes\", \"maxHeap\", \"extHeap\", \"maxStack\"]\n",
    "    speed_fieldnames = [\"Algorithm\", \"Operation\", \"\"]\n",
    "\n",
    "\n",
    "    # Looping throuhg the kem algorithms\n",
    "    for kem_alg in kem_algs:\n",
    "\n",
    "        # Looping throuhg each run\n",
    "        for run_count in range (1,16,1):\n",
    "\n",
    "            # Creating filenames and dataframe\n",
    "            kem_mem_filename = kem_mem_file_prefix + str(run_count) + \".csv\"\n",
    "            kem_speed_filename = sig_mem_file_prefix + str(run_count) + \".csv\"\n",
    "\n",
    "            # # Loading current file into temp dataframe\n",
    "            # temp_df = pd.read_csv(kem_speed_filename)\n",
    "            # print(kem_alg)\n",
    "            # print(temp_df[\"Iterations\"])\n",
    "            # break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"****************************************************\"\"\"\n",
    "    \"\"\"MESSAGE DYLAN BACK ABOUT NUMBER AND ALSO ANDREW!!!!!\"\"\"\n",
    "    \"\"\"****************************************************\"\"\"\n",
    "    \n",
    "\n",
    "#***********************************************************************\n",
    "def process_tests(num_machines):\n",
    "    \"\"\"This function parases the results for multiple machines and stores them as csv files\"\"\"\n",
    "\n",
    "    # Declaring directory variables\n",
    "    results_dir = root_dir + \"results/\"\n",
    "    mem_dir = results_dir + \"liboqs/\" + \"mem-results/\"\n",
    "    speed_dir = results_dir + \"liboqs/\" + \"speed-results/\"\n",
    "    up_mem = root_dir + \"up-results/liboqs/mem-results/\"\n",
    "    up_speed = root_dir + \"up-results/liboqs/speed-results/\"\n",
    "    num_mach_range = num_machines + 1\n",
    "\n",
    "    # Creating directory structure and remvoing previous results\n",
    "    try: \n",
    "\n",
    "        # Making results direcotry structure\n",
    "        os.makedirs(mem_dir)\n",
    "        os.makedirs(speed_dir)\n",
    "    \n",
    "    except:\n",
    "\n",
    "        # Removing the previous results\n",
    "        shutil.rmtree(mem_dir)\n",
    "        shutil.rmtree(speed_dir)\n",
    "        os.makedirs(mem_dir)\n",
    "        os.makedirs(speed_dir)\n",
    "\n",
    "\n",
    "    for machine_num in range(1, num_mach_range, 1):\n",
    "\n",
    "        type_name = \"machine-\" + str(machine_num) + \"/\"\n",
    "\n",
    "        # Setting up directory path\n",
    "        up_speed_dir = up_speed + type_name\n",
    "        up_mem_dir = up_mem + type_name\n",
    "\n",
    "        # Creating specifc result directories and clearing old results\n",
    "        try: \n",
    "            \n",
    "            # Speed result directories\n",
    "            type_speed_dir = speed_dir + type_name\n",
    "            os.makedirs(type_speed_dir)\n",
    "\n",
    "            # Mem result directories\n",
    "            type_mem_dir = mem_dir + type_name\n",
    "            os.makedirs(type_mem_dir)\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Setting the directory variables\n",
    "            type_speed_dir = speed_dir + type_name\n",
    "            type_mem_dir = mem_dir + type_name\n",
    "\n",
    "            #Clearing the old results and making directories\n",
    "            shutil.rmtree(type_speed_dir)\n",
    "            shutil.rmtree(type_mem_dir)\n",
    "            os.makedirs(type_speed_dir)\n",
    "            os.makedirs(type_mem_dir)\n",
    "\n",
    "        # Parsing results\n",
    "        speed_processing(type_speed_dir, up_speed_dir)\n",
    "        memory_processing(type_mem_dir, up_mem_dir)\n",
    "        gen_averages(type_speed_dir, type_mem_dir)\n",
    "\n",
    "\n",
    "#***********************************************************************  \n",
    "def main():\n",
    "    \"\"\"Main function for parsing the test results\"\"\"\n",
    "\n",
    "    print(\"Preparing to Parse Results:\\n\")\n",
    "\n",
    "    # Creating the algorithms list\n",
    "    get_algs()\n",
    "\n",
    "    # Getting the number of machines tested\n",
    "    prompt_flag = 0\n",
    "    \n",
    "    while prompt_flag == 0:\n",
    "        try:\n",
    "            machine_num = int(input(\"Enter the number of machines tested - \"))\n",
    "            prompt_flag = 1\n",
    "        except ValueError:\n",
    "            print(\"Invlaid Input - Please enter a number! - \")\n",
    "\n",
    "    # Processing the results\n",
    "    print(\"Parsing results... \")\n",
    "    process_tests(machine_num)\n",
    "    print(f\"\\nResults have been processed - CSV files can be found in the Results Directory at the repo root\\n\")\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "\"\"\"Main boiler plate\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
