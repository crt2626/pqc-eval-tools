{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to Parse Results:\n",
      "\n",
      "['BIKE-L1', 'BIKE-L3', 'BIKE-L5', 'Classic-McEliece-348864', 'Classic-McEliece-348864f', 'Classic-McEliece-460896', 'Classic-McEliece-460896f', 'Classic-McEliece-6688128', 'Classic-McEliece-6688128f', 'Classic-McEliece-6960119', 'Classic-McEliece-6960119f', 'Classic-McEliece-8192128', 'Classic-McEliece-8192128f', 'HQC-128', 'HQC-192', 'HQC-256', 'Kyber512', 'Kyber768', 'Kyber1024', 'Kyber512-90s', 'Kyber768-90s', 'Kyber1024-90s', 'sntrup761', 'FrodoKEM-640-AES', 'FrodoKEM-640-SHAKE', 'FrodoKEM-976-AES', 'FrodoKEM-976-SHAKE', 'FrodoKEM-1344-AES', 'FrodoKEM-1344-SHAKE']\n",
      "['Dilithium2', 'Dilithium3', 'Dilithium5', 'Dilithium2-AES', 'Dilithium3-AES', 'Dilithium5-AES', 'Falcon-512', 'Falcon-1024', 'SPHINCS+-Haraka-128f-robust', 'SPHINCS+-Haraka-128f-simple', 'SPHINCS+-Haraka-128s-robust', 'SPHINCS+-Haraka-128s-simple', 'SPHINCS+-Haraka-192f-robust', 'SPHINCS+-Haraka-192f-simple', 'SPHINCS+-Haraka-192s-robust', 'SPHINCS+-Haraka-192s-simple', 'SPHINCS+-Haraka-256f-robust', 'SPHINCS+-Haraka-256f-simple', 'SPHINCS+-Haraka-256s-robust', 'SPHINCS+-Haraka-256s-simple', 'SPHINCS+-SHA256-128f-robust', 'SPHINCS+-SHA256-128f-simple', 'SPHINCS+-SHA256-128s-robust', 'SPHINCS+-SHA256-128s-simple', 'SPHINCS+-SHA256-192f-robust', 'SPHINCS+-SHA256-192f-simple', 'SPHINCS+-SHA256-192s-robust', 'SPHINCS+-SHA256-192s-simple', 'SPHINCS+-SHA256-256f-robust', 'SPHINCS+-SHA256-256f-simple', 'SPHINCS+-SHA256-256s-robust', 'SPHINCS+-SHA256-256s-simple', 'SPHINCS+-SHAKE256-128f-robust', 'SPHINCS+-SHAKE256-128f-simple', 'SPHINCS+-SHAKE256-128s-robust', 'SPHINCS+-SHAKE256-128s-simple', 'SPHINCS+-SHAKE256-192f-robust', 'SPHINCS+-SHAKE256-192f-simple', 'SPHINCS+-SHAKE256-192s-robust', 'SPHINCS+-SHAKE256-192s-simple', 'SPHINCS+-SHAKE256-256f-robust', 'SPHINCS+-SHAKE256-256f-simple', 'SPHINCS+-SHAKE256-256s-robust', 'SPHINCS+-SHAKE256-256s-simple']\n",
      "Parsing results... \n",
      "\n",
      "Results have been processed - CSV files can be found in the Results Directory at the repo root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This python script will parse the results files outputed by the bash scripts so that be viewed in a easier format or further pasresed \n",
    "by python\"\"\"\n",
    "\n",
    "\"\"\"Importing modules and declaring Global Variables\"\"\"\n",
    "# Importing\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Declaring gloabl\n",
    "kem_algs = []\n",
    "sig_algs = []\n",
    "kem_operations = [\"keygen\", \"encaps\", \"decaps\"]\n",
    "sig_operations = [\"keypair\", \"sign\", \"verify\"]\n",
    "#root_dir = \"/pqc/pqc-eval-tools/\"\n",
    "root_dir = \"../../\"\n",
    "\n",
    "#***********************************************************************\n",
    "def get_algs():\n",
    "    \"\"\"Function for creating list of algorithms\"\"\"\n",
    "\n",
    "    # Setting alg text file directories\n",
    "    kem_algs_file = root_dir + \"result-processing/algs/kem-algs-list.txt\"\n",
    "    sig_algs_file = root_dir + \"result-processing/algs/sig-algs-list.txt\"\n",
    "\n",
    "    # # Getting the kem algs\n",
    "    with open(kem_algs_file, \"r\") as kem_file:\n",
    "\n",
    "        for line in kem_file:\n",
    "            #alg = line.strip()\n",
    "            kem_algs.append(line.strip())\n",
    "    \n",
    "    # Getting the digital siganture algorithms\n",
    "    with open(sig_algs_file, \"r\") as alg_file:\n",
    "\n",
    "        for line in alg_file:\n",
    "            #alg = line.strip()\n",
    "            sig_algs.append(line.strip())\n",
    "\n",
    "    print(kem_algs)\n",
    "    print(sig_algs)\n",
    "\n",
    "#***********************************************************************\n",
    "def speed_processing(type_speed_dir, up_speed_dir):\n",
    "    \"\"\"Importing and processing result files\"\"\"\n",
    "\n",
    "    # Declaring initial variables\n",
    "    kem_prefix = \"test-kem-speed-\"\n",
    "    sig_prefix = \"test-sig-speed-\"\n",
    "\n",
    "    # Creating algorithm list to insert into new column\n",
    "    new_col_kem = [alg for alg in kem_algs for i in range(3)]\n",
    "    new_col_sig = [alg for alg in sig_algs for i in range(3)]\n",
    "    \n",
    "        \n",
    "    # Reading the original csv files and formating\n",
    "    for file_count in range(1,16,1):\n",
    "\n",
    "        \"\"\"Formating Kem Files\"\"\"\n",
    "        # Loading kem file into dataframe\n",
    "        filename_kem_pre = up_speed_dir + kem_prefix + str(file_count) + \".csv\"\n",
    "        temp_df = pd.read_csv(filename_kem_pre, delimiter=\"|\", index_col=False)\n",
    "\n",
    "        # Striping trailing spaces and removing algorithms from Operation\n",
    "        temp_df.columns = [col.strip() for col in temp_df.columns]\n",
    "        temp_df = temp_df.loc[~temp_df['Operation'].str.strip().isin(kem_algs)]\n",
    "        temp_df = temp_df.applymap(lambda val: val.strip() if isinstance(val, str) else val)\n",
    "\n",
    "        # Inserting new algorithm column and outputing formated csv\n",
    "        temp_df.insert(0, \"Algorithm\", new_col_kem)\n",
    "        filename_kem = type_speed_dir + kem_prefix + str(file_count) + \".csv\"\n",
    "        temp_df.to_csv(filename_kem, index=False)\n",
    "\n",
    "\n",
    "        \"\"\"Formating Digital Signature Files\"\"\"\n",
    "        # Loading kem file into dataframe and stiping trailing space in columns headers\n",
    "        filename_sig_pre = up_speed_dir + sig_prefix + str(file_count) + \".csv\"\n",
    "        temp_df = pd.read_csv(filename_sig_pre, delimiter=\"|\", index_col=False)\n",
    "\n",
    "        # Striping trailing spaces and removing algorithms from Operation\n",
    "        temp_df.columns = [col.strip() for col in temp_df.columns]\n",
    "        temp_df = temp_df.loc[~temp_df['Operation'].str.strip().isin(sig_algs)]\n",
    "        temp_df = temp_df.applymap(lambda val: val.strip() if isinstance(val, str) else val)\n",
    "\n",
    "        # Inserting new column and outputting formated csv\n",
    "        temp_df.insert(0, 'Algorithm', new_col_sig)\n",
    "        filename_sig = type_speed_dir + sig_prefix + str(file_count) + \".csv\"\n",
    "        temp_df.to_csv(filename_sig, index=False)\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "def get_peak(mem_file, peak_metrics):\n",
    "    \"\"\"Takes the current massif.out file and gets the peak memory metrics. \n",
    "        It comes from the run_mem.py script found in OQS Profiling Project\n",
    "        https://github.com/open-quantum-safe/profiling\"\"\"\n",
    "\n",
    "    # Parsing function from OQS Profile Project\n",
    "    # Gets max memory metric from algorithm operation\n",
    "    peak = -1\n",
    "    with open(mem_file, \"r\") as lines:\n",
    "        for line in lines: \n",
    "            if line.startswith(\" Detailed snapshots: [\"):\n",
    "                match = re.search(r\"(\\d+) \\(peak\\).*\", line)\n",
    "                if match:\n",
    "                    peak = int(match.group(1))\n",
    "            if peak > 0:\n",
    "                if line.startswith('{: >3d}'.format(peak)): # remove \",\" and print all numbers except first:\n",
    "                    nl = line.replace(\",\", \"\")\n",
    "                    peak_metrics.extend(nl.split()[1:])\n",
    "                    # print(\" \".join(res))\n",
    "\n",
    "    return peak_metrics\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "def memory_processing(type_mem_dir, up_mem_dir):\n",
    "    \"\"\"Looping through all memory files and creating csv files\"\"\"\n",
    "\n",
    "    # Assigning directory varibales\n",
    "    kem_dir = up_mem_dir + \"kem-mem-metrics/\"\n",
    "    sig_dir = up_mem_dir + \"sig-mem-metrics/\"\n",
    "    kem_file_prefix = \"kem-mem-metrics\"\n",
    "    sig_file_prefix = \"sig-mem-metrics\"\n",
    "    new_row = []\n",
    "    peak_metrics = []\n",
    "\n",
    "    # file placeholders\n",
    "    filednames = [\"Algorithm\", \"Operation\", \"intits\", \"maxBytes\", \"maxHeap\", \"extHeap\", \"maxStack\"]\n",
    "\n",
    "\n",
    "    # Looping through each run count\n",
    "    for run_count in range(1,16,1):\n",
    "\n",
    "        # Creating temp dataframe\n",
    "        temp_df = pd.DataFrame(columns=filednames)\n",
    "        #Looping throuhg kem algorithms\n",
    "        for kem_alg in kem_algs:\n",
    "\n",
    "            kem_up_filename_pre = kem_dir + kem_file_prefix\n",
    "\n",
    "            #Looping the operations and adding to temp dataframe \n",
    "            for operation in range(0,3,1):\n",
    "\n",
    "                # Parsing metrics and adding results to dataframe row\n",
    "                kem_up_filename = kem_up_filename_pre + \"-\" + kem_alg + \"-\" + str(operation) + \"-\" + str(run_count) + \".txt\"\n",
    "                peak_metrics = get_peak(kem_up_filename, peak_metrics)\n",
    "                new_row.extend([kem_alg, kem_operations[operation]])\n",
    "                new_row.extend(peak_metrics)\n",
    "                \n",
    "                temp_df.loc[len(temp_df)] = new_row\n",
    "\n",
    "                # Clearing lists\n",
    "                peak_metrics.clear()\n",
    "                new_row.clear()\n",
    "\n",
    "        # Outputing kem csv file for this run\n",
    "        kem_filename = type_mem_dir + \"kem-mem-metrics-\" + str(run_count) + \".csv\"\n",
    "        temp_df.to_csv(kem_filename, index=False)\n",
    "\n",
    "\n",
    "        #Looping throuhg kem algorithms\n",
    "        for sig_alg in sig_algs:\n",
    "\n",
    "            sig_up_filename_pre = sig_dir + sig_file_prefix\n",
    "\n",
    "            #Looping the operations and adding to temp dataframe \n",
    "            for operation in range(0,3,1):\n",
    "\n",
    "                # Parsing metrics and adding results to dataframe row\n",
    "                sig_up_filename = sig_up_filename_pre + \"-\" + sig_alg + \"-\" + str(operation) + \"-\" + str(run_count) + \".txt\"\n",
    "                peak_metrics = get_peak(sig_up_filename, peak_metrics)   \n",
    "                new_row.extend((sig_alg, sig_operations[operation]))\n",
    "                new_row.extend(peak_metrics)\n",
    "                temp_df.loc[len(temp_df)] = new_row\n",
    "\n",
    "                # Clearing lists\n",
    "                peak_metrics.clear()\n",
    "                new_row.clear()\n",
    "\n",
    "        # Outputing digital signature csv file for this run\n",
    "        sig_filename = type_mem_dir + \"sig-mem-metrics-\" + str(run_count) + \".csv\"\n",
    "        temp_df.to_csv(sig_filename, index=False)\n",
    "\n",
    "\n",
    "#***********************************************************************  \n",
    "def gen_averages(type_speed_dir, type_mem_dir):\n",
    "    \"\"\"Function for generating averages csv files for all tests\"\"\"\n",
    "\n",
    "    # Declaring directories variables\n",
    "    kem_speed_file_prefix = type_speed_dir + \"test-kem-speed-\"\n",
    "    sig_speed_file_prefix = type_speed_dir + \"test-sig-speed-\"\n",
    "    kem_mem_file_prefix = type_mem_dir + \"kem-mem-metrics-\"\n",
    "    sig_mem_file_prefix = type_mem_dir + \"sig-mem-metrics-\"\n",
    "\n",
    "    # Declaring dataframes and fieldnames\n",
    "    mem_fieldnames = [\"Algorithm\", \"Operation\", \"intits\", \"maxBytes\", \"maxHeap\", \"extHeap\", \"maxStack\"]\n",
    "    speed_fieldnames = [\"Algorithm\", \"Operation\", \"\"]\n",
    "\n",
    "\n",
    "    # Looping throuhg the kem algorithms\n",
    "    for kem_alg in kem_algs:\n",
    "\n",
    "        # Looping throuhg each run\n",
    "        for run_count in range (1,16,1):\n",
    "\n",
    "            # Creating filenames and dataframe\n",
    "            kem_mem_filename = kem_mem_file_prefix + str(run_count) + \".csv\"\n",
    "            kem_speed_filename = sig_mem_file_prefix + str(run_count) + \".csv\"\n",
    "\n",
    "            # # Loading current file into temp dataframe\n",
    "            # temp_df = pd.read_csv(kem_speed_filename)\n",
    "            # print(kem_alg)\n",
    "            # print(temp_df[\"Iterations\"])\n",
    "            # break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"****************************************************\"\"\"\n",
    "    \"\"\"MESSAGE DYLAN BACK ABOUT NUMBER AND ALSO ANDREW!!!!!\"\"\"\n",
    "    \"\"\"****************************************************\"\"\"\n",
    "    \n",
    "\n",
    "#***********************************************************************\n",
    "def process_tests(num_machines):\n",
    "    \"\"\"This function parases the results for multiple machines and stores them as csv files\"\"\"\n",
    "\n",
    "    # Declaring directory variables\n",
    "    results_dir = root_dir + \"results/\"\n",
    "    mem_dir = results_dir + \"liboqs/\" + \"mem-results/\"\n",
    "    speed_dir = results_dir + \"liboqs/\" + \"speed-results/\"\n",
    "    up_mem = root_dir + \"up-results/liboqs/mem-results/\"\n",
    "    up_speed = root_dir + \"up-results/liboqs/speed-results/\"\n",
    "    num_mach_range = num_machines + 1\n",
    "\n",
    "    # Creating directory structure and remvoing previous results\n",
    "    try: \n",
    "\n",
    "        # Making results direcotry structure\n",
    "        os.makedirs(mem_dir)\n",
    "        os.makedirs(speed_dir)\n",
    "    \n",
    "    except:\n",
    "\n",
    "        # Removing the previous results\n",
    "        shutil.rmtree(mem_dir)\n",
    "        shutil.rmtree(speed_dir)\n",
    "        os.makedirs(mem_dir)\n",
    "        os.makedirs(speed_dir)\n",
    "\n",
    "\n",
    "    for machine_num in range(1, num_mach_range, 1):\n",
    "\n",
    "        type_name = \"machine-\" + str(machine_num) + \"/\"\n",
    "\n",
    "        # Setting up directory path\n",
    "        up_speed_dir = up_speed + type_name\n",
    "        up_mem_dir = up_mem + type_name\n",
    "\n",
    "        # Creating specifc result directories and clearing old results\n",
    "        try: \n",
    "            \n",
    "            # Speed result directories\n",
    "            type_speed_dir = speed_dir + type_name\n",
    "            os.makedirs(type_speed_dir)\n",
    "\n",
    "            # Mem result directories\n",
    "            type_mem_dir = mem_dir + type_name\n",
    "            os.makedirs(type_mem_dir)\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Setting the directory variables\n",
    "            type_speed_dir = speed_dir + type_name\n",
    "            type_mem_dir = mem_dir + type_name\n",
    "\n",
    "            #Clearing the old results and making directories\n",
    "            shutil.rmtree(type_speed_dir)\n",
    "            shutil.rmtree(type_mem_dir)\n",
    "            os.makedirs(type_speed_dir)\n",
    "            os.makedirs(type_mem_dir)\n",
    "\n",
    "        # Parsing results\n",
    "        speed_processing(type_speed_dir, up_speed_dir)\n",
    "        memory_processing(type_mem_dir, up_mem_dir)\n",
    "        gen_averages(type_speed_dir, type_mem_dir)\n",
    "\n",
    "\n",
    "#***********************************************************************  \n",
    "def main():\n",
    "    \"\"\"Main function for parsing the test results\"\"\"\n",
    "\n",
    "    print(\"Preparing to Parse Results:\\n\")\n",
    "\n",
    "    # Creating the algorithms list\n",
    "    get_algs()\n",
    "\n",
    "    # Getting the number of machines tested\n",
    "    prompt_flag = 0\n",
    "    \n",
    "    while prompt_flag == 0:\n",
    "        try:\n",
    "            machine_num = int(input(\"Enter the number of machines tested - \"))\n",
    "            prompt_flag = 1\n",
    "        except ValueError:\n",
    "            print(\"Invlaid Input - Please enter a number! - \")\n",
    "\n",
    "    # Processing the results\n",
    "    print(\"Parsing results... \")\n",
    "    process_tests(machine_num)\n",
    "    print(f\"\\nResults have been processed - CSV files can be found in the Results Directory at the repo root\\n\")\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "\"\"\"Main boiler plate\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
