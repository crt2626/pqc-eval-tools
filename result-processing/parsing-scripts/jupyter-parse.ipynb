{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to Parse Results:\n",
      "\n",
      "Parsing results... \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (87)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 321\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39m\"\"\"Main boiler plate\"\"\"\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 321\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[68], line 314\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39m# Processing the results\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mParsing results... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 314\u001b[0m process_tests(machine_num)\n\u001b[0;32m    315\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mResults have been processed - CSV files can be found in the Results Directory at the repo root\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[68], line 288\u001b[0m, in \u001b[0;36mprocess_tests\u001b[1;34m(num_machines)\u001b[0m\n\u001b[0;32m    285\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(type_mem_dir)\n\u001b[0;32m    287\u001b[0m \u001b[39m# Parsing results\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m speed_processing(type_speed_dir, up_speed_dir)\n\u001b[0;32m    289\u001b[0m memory_processing(type_mem_dir, up_mem_dir)\n\u001b[0;32m    290\u001b[0m gen_averages(type_speed_dir, type_mem_dir)\n",
      "Cell \u001b[1;32mIn[68], line 69\u001b[0m, in \u001b[0;36mspeed_processing\u001b[1;34m(type_speed_dir, up_speed_dir)\u001b[0m\n\u001b[0;32m     66\u001b[0m         temp_df\u001b[39m.\u001b[39mdrop(index, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m \u001b[39m# Inserting new column and outputing formated csv\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m temp_df\u001b[39m.\u001b[39;49minsert(\u001b[39m0\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mAlgorithm\u001b[39;49m\u001b[39m\"\u001b[39;49m, new_col_kem)\n\u001b[0;32m     70\u001b[0m filename_kem \u001b[39m=\u001b[39m type_speed_dir \u001b[39m+\u001b[39m kem_prefix \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(file_count) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m temp_df\u001b[39m.\u001b[39mto_csv(filename_kem, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4821\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[1;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   4818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, \u001b[39mint\u001b[39m):\n\u001b[0;32m   4819\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mloc must be int\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 4821\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   4822\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39minsert(loc, column, value)\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4912\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4914\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4915\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4916\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\site-packages\\pandas\\core\\common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 571\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (0) does not match length of index (87)"
     ]
    }
   ],
   "source": [
    "\"\"\"This python script will parse the results files outputed by the bash scripts so that be viewed in a easier format or further pasresed \n",
    "by python\"\"\"\n",
    "\n",
    "\"\"\"Importing modules and declaring Global Variables\"\"\"\n",
    "# Importing\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Declaring gloabl\n",
    "kem_algs = []\n",
    "sig_algs = []\n",
    "kem_operations = [\"keygen\", \"encaps\", \"decaps\"]\n",
    "sig_operations = [\"keypair\", \"sign\", \"verify\"]\n",
    "#root_dir = \"/pqc/pqc-eval-tools/\"\n",
    "root_dir = \"../../\"\n",
    "\n",
    "#***********************************************************************\n",
    "def get_algs():\n",
    "    \"\"\"Function for creating list of algorithms\"\"\"\n",
    "\n",
    "    # Setting alg text file directories\n",
    "    kem_algs_file = root_dir + \"result-processing/algs/kem-algs-list.txt\"\n",
    "    sig_algs_file = root_dir + \"result-processing/algs/sig-algs-list.txt\"\n",
    "\n",
    "    # Getting the kem algs\n",
    "    with open(kem_algs_file, \"r\") as kem_file:\n",
    "\n",
    "        for line in kem_file:\n",
    "            line = line.strip()\n",
    "            kem_algs.append(line)\n",
    "    \n",
    "    # Getting the digital siganture algorithms\n",
    "    with open(sig_algs_file, \"r\") as alg_file:\n",
    "\n",
    "        for line in alg_file:\n",
    "            line = line.strip()\n",
    "            sig_algs.append(line)\n",
    "\n",
    "    print(kem_algs)\n",
    "    print(sig_algs)\n",
    "\n",
    "#***********************************************************************\n",
    "def speed_processing(type_speed_dir, up_speed_dir):\n",
    "    \"\"\"Importing and processing result files\"\"\"\n",
    "\n",
    "    # Declaring initial variables\n",
    "    kem_prefix = \"test-kem-speed-\"\n",
    "    sig_prefix = \"test-sig-speed-\"\n",
    "\n",
    "    # Creating algorithm list to insert into new column\n",
    "    new_col_kem = [alg for alg in kem_algs for i in range(3)]\n",
    "    new_col_sig = [alg for alg in sig_algs for i in range(3)]\n",
    "\n",
    "    # Reading the original csv files and formating\n",
    "    for file_count in range(1,16,1):\n",
    "\n",
    "        \"\"\"Formating Kem Files\"\"\"\n",
    "        # Loading kem file into dataframe and stiping trailing space in columns headers\n",
    "        filename_kem_pre = up_speed_dir + kem_prefix + str(file_count) + \".csv\"\n",
    "        temp_df = pd.read_csv(filename_kem_pre, delimiter=\"|\", index_col=False)\n",
    "        temp_df.columns = [col.strip() for col in temp_df.columns]\n",
    "\n",
    "        # Removing any now that is not a kem operation\n",
    "        for index, row in temp_df.iterrows():\n",
    "            if row['Operation'].strip() not in kem_operations:\n",
    "                temp_df.drop(index, inplace=True)\n",
    "        \n",
    "        # Inserting new column and outputing formated csv\n",
    "        temp_df.insert(0, \"Algorithm\", new_col_kem)\n",
    "        filename_kem = type_speed_dir + kem_prefix + str(file_count) + \".csv\"\n",
    "        temp_df.to_csv(filename_kem, index=False)\n",
    "\n",
    "        \"\"\"Formating Digital Signature Files\"\"\"\n",
    "        # Loading kem file into dataframe and stiping trailing space in columns headers\n",
    "        filename_sig_pre = up_speed_dir + sig_prefix + str(file_count) + \".csv\"\n",
    "        temp_df = pd.read_csv(filename_sig_pre, delimiter=\"|\", index_col=False)\n",
    "        temp_df.columns = [col.strip() for col in temp_df.columns]\n",
    "\n",
    "        temp_df.to_csv('./test.csv', index=False)        \n",
    "\n",
    "        for index, row in temp_df.iterrows():\n",
    "            row_str = row['Operation']\n",
    "            if row['Operation'].strip() not in sig_operations:\n",
    "                temp_df.drop(index, inplace=True)\n",
    "\n",
    "\n",
    "        # Inserting new column and outputting formated csv\n",
    "        temp_df.insert(0, 'Algorithm', new_col_sig)\n",
    "        filename_sig = type_speed_dir + sig_prefix + str(file_count) + \".csv\"\n",
    "        temp_df.to_csv(filename_sig, index=False)\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "def get_peak(mem_file, peak_metrics):\n",
    "    \"\"\"Takes the current massif.out file and gets the peak memory metrics. \n",
    "        It comes from the run_mem.py script found in OQS Profiling Project\n",
    "        https://github.com/open-quantum-safe/profiling\"\"\"\n",
    "\n",
    "    # Parsing function from OQS Profile Project\n",
    "    # Gets max memory metric from algorithm operation\n",
    "    peak = -1\n",
    "    with open(mem_file, \"r\") as lines:\n",
    "        for line in lines: \n",
    "            if line.startswith(\" Detailed snapshots: [\"):\n",
    "                match = re.search(r\"(\\d+) \\(peak\\).*\", line)\n",
    "                if match:\n",
    "                    peak = int(match.group(1))\n",
    "            if peak > 0:\n",
    "                if line.startswith('{: >3d}'.format(peak)): # remove \",\" and print all numbers except first:\n",
    "                    nl = line.replace(\",\", \"\")\n",
    "                    peak_metrics.extend(nl.split()[1:])\n",
    "                    # print(\" \".join(res))\n",
    "\n",
    "    return peak_metrics\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "def memory_processing(type_mem_dir, up_mem_dir):\n",
    "    \"\"\"Looping through all memory files and creating csv files\"\"\"\n",
    "\n",
    "    # Assigning directory varibales\n",
    "    kem_dir = up_mem_dir + \"kem-mem-metrics/\"\n",
    "    sig_dir = up_mem_dir + \"sig-mem-metrics/\"\n",
    "    kem_file_prefix = \"kem-mem-metrics\"\n",
    "    sig_file_prefix = \"sig-mem-metrics\"\n",
    "    new_row = []\n",
    "    peak_metrics = []\n",
    "\n",
    "    # file placeholders\n",
    "    filednames = [\"Algorithm\", \"Operation\", \"intits\", \"maxBytes\", \"maxHeap\", \"extHeap\", \"maxStack\"]\n",
    "\n",
    "\n",
    "    # Looping through each run count\n",
    "    for run_count in range(1,16,1):\n",
    "\n",
    "        # Creating temp dataframe\n",
    "        temp_df = pd.DataFrame(columns=filednames)\n",
    "        #Looping throuhg kem algorithms\n",
    "        for kem_alg in kem_algs:\n",
    "\n",
    "            kem_up_filename_pre = kem_dir + kem_file_prefix\n",
    "\n",
    "            #Looping the operations and adding to temp dataframe \n",
    "            for operation in range(0,3,1):\n",
    "\n",
    "                # Parsing metrics and adding results to dataframe row\n",
    "                kem_up_filename = kem_up_filename_pre + \"-\" + kem_alg + \"-\" + str(operation) + \"-\" + str(run_count) + \".txt\"\n",
    "                peak_metrics = get_peak(kem_up_filename, peak_metrics)\n",
    "                new_row.extend([kem_alg, kem_operations[operation]])\n",
    "                new_row.extend(peak_metrics)\n",
    "                \n",
    "                temp_df.loc[len(temp_df)] = new_row\n",
    "\n",
    "                # Clearing lists\n",
    "                peak_metrics.clear()\n",
    "                new_row.clear()\n",
    "\n",
    "        # Outputing kem csv file for this run\n",
    "        kem_filename = type_mem_dir + \"kem-mem-metrics-\" + str(run_count) + \".csv\"\n",
    "        temp_df.to_csv(kem_filename, index=False)\n",
    "\n",
    "\n",
    "        #Looping throuhg kem algorithms\n",
    "        for sig_alg in sig_algs:\n",
    "\n",
    "            sig_up_filename_pre = sig_dir + sig_file_prefix\n",
    "\n",
    "            #Looping the operations and adding to temp dataframe \n",
    "            for operation in range(0,3,1):\n",
    "\n",
    "                # Parsing metrics and adding results to dataframe row\n",
    "                sig_up_filename = sig_up_filename_pre + \"-\" + sig_alg + \"-\" + str(operation) + \"-\" + str(run_count) + \".txt\"\n",
    "                peak_metrics = get_peak(sig_up_filename, peak_metrics)   \n",
    "                new_row.extend((sig_alg, sig_operations[operation]))\n",
    "                new_row.extend(peak_metrics)\n",
    "                temp_df.loc[len(temp_df)] = new_row\n",
    "\n",
    "                # Clearing lists\n",
    "                peak_metrics.clear()\n",
    "                new_row.clear()\n",
    "\n",
    "        # Outputing digital signature csv file for this run\n",
    "        sig_filename = type_mem_dir + \"sig-mem-metrics-\" + str(run_count) + \".csv\"\n",
    "        temp_df.to_csv(sig_filename, index=False)\n",
    "\n",
    "\n",
    "#***********************************************************************  \n",
    "def gen_averages(type_speed_dir, type_mem_dir):\n",
    "    \"\"\"Function for generating averages csv files for all tests\"\"\"\n",
    "\n",
    "    # Declaring directories variables\n",
    "    kem_speed_file_prefix = type_speed_dir + \"test-kem-speed-\"\n",
    "    sig_speed_file_prefix = type_speed_dir + \"test-sig-speed-\"\n",
    "    kem_mem_file_prefix = type_mem_dir + \"kem-mem-metrics-\"\n",
    "    sig_mem_file_prefix = type_mem_dir + \"sig-mem-metrics-\"\n",
    "\n",
    "    # Declaring dataframes and fieldnames\n",
    "    #mem_fieldnames = [\"Algorithm\", \"Operation\", \"intits\", \"maxBytes\", \"maxHeap\", \"extHeap\", \"maxStack\"]\n",
    "    speed_fieldnames = [\"Algorithm\", \"Operation\", \"\"]\n",
    "\n",
    "\n",
    "    # Looping throuhg the kem algorithms\n",
    "    for kem_alg in kem_algs:\n",
    "\n",
    "        # Looping throuhg each run\n",
    "        for run_count in range (1,16,1):\n",
    "\n",
    "            # Creating filenames and dataframe\n",
    "            kem_mem_filename = kem_mem_file_prefix + str(run_count) + \".csv\"\n",
    "            kem_speed_filename = sig_mem_file_prefix + str(run_count) + \".csv\"\n",
    "\n",
    "            # # Loading current file into temp dataframe\n",
    "            # temp_df = pd.read_csv(kem_speed_filename)\n",
    "            # print(kem_alg)\n",
    "            # print(temp_df[\"Iterations\"])\n",
    "            # break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"****************************************************\"\"\"\n",
    "    \"\"\"MESSAGE DYLAN BACK ABOUT NUMBER AND ALSO ANDREW!!!!!\"\"\"\n",
    "    \"\"\"****************************************************\"\"\"\n",
    "    \n",
    "\n",
    "#***********************************************************************\n",
    "def process_tests(num_machines):\n",
    "    \"\"\"This function parases the results for multiple machines and stores them as csv files\"\"\"\n",
    "\n",
    "    # Declaring directory variables\n",
    "    results_dir = root_dir + \"results/\"\n",
    "    mem_dir = results_dir + \"liboqs/\" + \"mem-results/\"\n",
    "    speed_dir = results_dir + \"liboqs/\" + \"speed-results/\"\n",
    "    up_mem = root_dir + \"up-results/liboqs/mem-results/\"\n",
    "    up_speed = root_dir + \"up-results/liboqs/speed-results/\"\n",
    "    num_mach_range = num_machines + 1\n",
    "\n",
    "    # Creating directory structure and remvoing previous results\n",
    "    try: \n",
    "\n",
    "        # Making results direcotry structure\n",
    "        os.makedirs(mem_dir)\n",
    "        os.makedirs(speed_dir)\n",
    "    \n",
    "    except:\n",
    "\n",
    "        # Removing the previous results\n",
    "        shutil.rmtree(mem_dir)\n",
    "        shutil.rmtree(speed_dir)\n",
    "        os.makedirs(mem_dir)\n",
    "        os.makedirs(speed_dir)\n",
    "\n",
    "\n",
    "    for machine_num in range(1, num_mach_range, 1):\n",
    "\n",
    "        type_name = \"machine-\" + str(machine_num) + \"/\"\n",
    "\n",
    "        # Setting up directory path\n",
    "        up_speed_dir = up_speed + type_name\n",
    "        up_mem_dir = up_mem + type_name\n",
    "\n",
    "        # Creating specifc result directories and clearing old results\n",
    "        try: \n",
    "            \n",
    "            # Speed result directories\n",
    "            type_speed_dir = speed_dir + type_name\n",
    "            os.makedirs(type_speed_dir)\n",
    "\n",
    "            # Mem result directories\n",
    "            type_mem_dir = mem_dir + type_name\n",
    "            os.makedirs(type_mem_dir)\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Setting the directory variables\n",
    "            type_speed_dir = speed_dir + type_name\n",
    "            type_mem_dir = mem_dir + type_name\n",
    "\n",
    "            #Clearing the old results and making directories\n",
    "            shutil.rmtree(type_speed_dir)\n",
    "            shutil.rmtree(type_mem_dir)\n",
    "            os.makedirs(type_speed_dir)\n",
    "            os.makedirs(type_mem_dir)\n",
    "\n",
    "        # Parsing results\n",
    "        speed_processing(type_speed_dir, up_speed_dir)\n",
    "        memory_processing(type_mem_dir, up_mem_dir)\n",
    "        gen_averages(type_speed_dir, type_mem_dir)\n",
    "\n",
    "\n",
    "#***********************************************************************  \n",
    "def main():\n",
    "    \"\"\"Main function for parsing the test results\"\"\"\n",
    "\n",
    "    print(\"Preparing to Parse Results:\\n\")\n",
    "\n",
    "    # Creating the algorithms list\n",
    "    get_algs()\n",
    "\n",
    "    # Getting the number of machines tested\n",
    "    prompt_flag = 0\n",
    "    \n",
    "    while prompt_flag == 0:\n",
    "        try:\n",
    "            machine_num = int(input(\"Enter the number of machines tested - \"))\n",
    "            prompt_flag = 1\n",
    "        except ValueError:\n",
    "            print(\"Invlaid Input - Please enter a number! - \")\n",
    "\n",
    "    # Processing the results\n",
    "    print(\"Parsing results... \")\n",
    "    process_tests(machine_num)\n",
    "    print(f\"\\nResults have been processed - CSV files can be found in the Results Directory at the repo root\\n\")\n",
    "\n",
    "\n",
    "#***********************************************************************\n",
    "\"\"\"Main boiler plate\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
